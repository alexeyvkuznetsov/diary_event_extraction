# app.py
import streamlit as st
import pandas as pd
from ast import literal_eval # –î–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ø–∏—Å–∫–æ–≤ –∏–∑ —Å—Ç—Ä–æ–∫
import os # –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
import json # –î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ö–∞—Ä—Ç—ã –ó–Ω–∞–Ω–∏–π –∏–∑ —Å—Ç—Ä–æ–∫–∏

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã ---
# st.set_page_config –î–û–õ–ñ–ù–ê –±—ã—Ç—å –ø–µ—Ä–≤–æ–π –∫–æ–º–∞–Ω–¥–æ–π Streamlit
st.set_page_config(
    layout="wide", # –ò—Å–ø–æ–ª—å–∑—É–µ–º —à–∏—Ä–æ–∫—É—é —Ä–∞—Å–∫–ª–∞–¥–∫—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    page_title="–ê–Ω–∞–ª–∏–∑ –¥–Ω–µ–≤–Ω–∏–∫–∞ –≥–∏–º–Ω–∞–∑–∏—Å—Ç–∞ v3.3", # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Ä—Å–∏—é
    page_icon="üìú" # –ò–∫–æ–Ω–∫–∞ –≤–æ –≤–∫–ª–∞–¥–∫–µ –±—Ä–∞—É–∑–µ—Ä–∞
)

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—É—Ç–µ–π ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤
DATA_PATH_PKL = os.path.join(BASE_DIR, 'merged_diary_events_v3.pkl')
DATA_PATH_CSV = os.path.join(BASE_DIR, 'merged_diary_events_v3.csv')

# --- –ö–∞—Ä—Ç–∞ –ó–Ω–∞–Ω–∏–π (–≤—Å—Ç—Ä–æ–µ–Ω–∞ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞, –ª—É—á—à–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å –∏–∑ —Ñ–∞–π–ª–∞) ---
KNOWLEDGE_MAP_JSON = """
[
  {
    "category_id": "REV1848", "category_name": "–†–µ–≤–æ–ª—é—Ü–∏–∏ –∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã 1848-1849 –≥–≥.",
    "subcategories": [
      {"subcategory_id": "REV1848_HUN", "subcategory_name": "–í–µ–Ω–≥–µ—Ä—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–∏–µ 1848-1849 –≥–≥.", "events": [
          {"event_id": "REV1848_HUN_INT", "event_name": "–†–æ—Å—Å–∏–π—Å–∫–∞—è –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏—è –≤ –í–µ–Ω–≥—Ä–∏–∏ (1849)"},
          {"event_id": "REV1848_HUN_MIL", "event_name": "–í–æ–µ–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è (–í–µ–Ω–≥—Ä–∏—è, –æ–±—â–µ–µ)"},
          {"event_id": "REV1848_HUN_CAP", "event_name": "–ö–∞–ø–∏—Ç—É–ª—è—Ü–∏—è –≤–µ–Ω–≥–µ—Ä—Å–∫–æ–π –∞—Ä–º–∏–∏ (–í–∏–ª–∞–≥–æ—à)"},
          {"event_id": "REV1848_HUN_FIG", "event_name": "–£–ø–æ–º–∏–Ω–∞–Ω–∏—è –ª–∏—á–Ω–æ—Å—Ç–µ–π (–ë–µ–º, –ü–∞—Å–∫–µ–≤–∏—á)"} ]},
      {"subcategory_id": "REV1848_ITA", "subcategory_name": "–†–µ–≤–æ–ª—é—Ü–∏–∏ –≤ –ò—Ç–∞–ª–∏–∏", "events": [
          {"event_id": "REV1848_ITA_ROM", "event_name": "–†–∏–º—Å–∫–∞—è —Ä–µ—Å–ø—É–±–ª–∏–∫–∞ (1849)"},
          {"event_id": "REV1848_ITA_AUS", "event_name": "–ê–≤—Å—Ç—Ä–æ-—Å–∞—Ä–¥–∏–Ω—Å–∫–∞—è –≤–æ–π–Ω–∞ (1848-1849)"} ]},
      {"subcategory_id": "REV1848_GER", "subcategory_name": "–†–µ–≤–æ–ª—é—Ü–∏–∏ –≤ –ì–µ—Ä–º–∞–Ω–∏–∏/–ü—Ä—É—Å—Å–∏–∏ (–æ–±—â–µ–µ)"},
      {"subcategory_id": "REV1848_FRA", "subcategory_name": "–†–µ–≤–æ–ª—é—Ü–∏–∏ –≤–æ –§—Ä–∞–Ω—Ü–∏–∏ (–æ–±—â–µ–µ)"},
      {"subcategory_id": "REV1848_EUR", "subcategory_name": "–û–±—â–µ–µ–≤—Ä–æ–ø–µ–π—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç/–ë–µ—Å–ø–æ—Ä—è–¥–∫–∏ –Ω–∞ –ó–∞–ø–∞–¥–µ"}
    ]
  },
  {
    "category_id": "EPIDEMIC", "category_name": "–≠–ø–∏–¥–µ–º–∏–∏ –∏ –∑–¥–æ—Ä–æ–≤—å–µ",
    "subcategories": [
       {"subcategory_id": "EPIDEMIC_CHOLERA", "subcategory_name": "–≠–ø–∏–¥–µ–º–∏—è —Ö–æ–ª–µ—Ä—ã (1848-1849)", "events": [
            {"event_id": "EPIDEMIC_CHOLERA_SPB", "event_name": "–•–æ–ª–µ—Ä–∞ –≤ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–µ"},
            {"event_id": "EPIDEMIC_CHOLERA_NOV", "event_name": "–•–æ–ª–µ—Ä–∞ –≤ –ù–æ–≤–≥–æ—Ä–æ–¥—Å–∫–æ–π –≥—É–±–µ—Ä–Ω–∏–∏"},
            {"event_id": "EPIDEMIC_CHOLERA_VOL", "event_name": "–•–æ–ª–µ—Ä–∞ –≤ –í–æ–ª–æ–≥–¥–µ (—Å–ª—É—Ö–∏/—É–ø–æ–º–∏–Ω–∞–Ω–∏—è)"},
            {"event_id": "EPIDEMIC_CHOLERA_DTH", "event_name": "–°–º–µ—Ä—Ç—å –æ—Ç —Ö–æ–ª–µ—Ä—ã"},
            {"event_id": "EPIDEMIC_CHOLERA_FEAR", "event_name": "–û–ø–∞—Å–µ–Ω–∏—è/–°—Ç—Ä–∞—Ö –ø–µ—Ä–µ–¥ —Ö–æ–ª–µ—Ä–æ–π"} ]},
       {"subcategory_id": "EPIDEMIC_OTH", "subcategory_name": "–î—Ä—É–≥–∏–µ –±–æ–ª–µ–∑–Ω–∏"}
    ]
  },
  {
    "category_id": "RU_POLITICS", "category_name": "–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –ø–æ–ª–∏—Ç–∏–∫–∞ –∏ –æ–±—â–µ—Å—Ç–≤–æ –†–æ—Å—Å–∏–π—Å–∫–æ–π –ò–º–ø–µ—Ä–∏–∏",
    "subcategories": [
      {"subcategory_id": "RU_POLITICS_REPRESS", "subcategory_name": "–†–µ–ø—Ä–µ—Å—Å–∏–∏ –∏ —Ü–µ–Ω–∑—É—Ä–∞ (–ù–∏–∫–æ–ª–∞–π I)", "events": [
            {"event_id": "RU_POLITICS_REPRESS_PETR", "event_name": "–î–µ–ª–æ –ø–µ—Ç—Ä–∞—à–µ–≤—Ü–µ–≤"},
            {"event_id": "RU_POLITICS_REPRESS_CENS", "event_name": "–£—Å–∏–ª–µ–Ω–∏–µ —Ü–µ–Ω–∑—É—Ä—ã –ø–µ—á–∞—Ç–∏"},
            {"event_id": "RU_POLITICS_REPRESS_SHAVE", "event_name": "–£–∫–∞–∑ –æ–± –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–º –±—Ä–∏—Ç—å–µ"},
            {"event_id": "RU_POLITICS_REPRESS_POLICE", "event_name": "–£–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ç–∞–π–Ω–æ–π –ø–æ–ª–∏—Ü–∏–∏"} ]},
      {"subcategory_id": "RU_POLITICS_IMPERIAL", "subcategory_name": "–ò–º–ø–µ—Ä–∞—Ç–æ—Ä—Å–∫–∞—è —Å–µ–º—å—è –∏ –¥–≤–æ—Ä", "events": [
           {"event_id": "RU_POLITICS_IMPERIAL_CORON", "event_name": "–ì–æ–¥–æ–≤—â–∏–Ω–∞ –∫–æ—Ä–æ–Ω–∞—Ü–∏–∏ –ù–∏–∫–æ–ª–∞—è I"},
           {"event_id": "RU_POLITICS_IMPERIAL_DEATH", "event_name": "–°–º–µ—Ä—Ç—å –í–µ–ª–∏–∫–æ–≥–æ –ö–Ω—è–∑—è –ú–∏—Ö–∞–∏–ª–∞ –ü–∞–≤–ª–æ–≤–∏—á–∞"} ]},
       {"subcategory_id": "RU_POLITICS_SOCIAL", "subcategory_name": "–°–æ—Ü–∏–∞–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã/–Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è", "events": [
           {"event_id": "RU_POLITICS_SOCIAL_REVMOOD", "event_name": "–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –º–æ–ª–æ–¥–µ–∂–∏"} ]}
    ]
  },
   {
    "category_id": "RU_FOREIGN", "category_name": "–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è (–≤–Ω–µ 1848)",
    "subcategories": [
      {"subcategory_id": "RU_FOREIGN_TURK", "subcategory_name": "–†–æ—Å—Å–∏–π—Å–∫–æ-—Ç—É—Ä–µ—Ü–∫–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è", "events": [
           {"event_id": "RU_FOREIGN_TURK_WAR", "event_name": "–°–ª—É—Ö–∏ –æ –≤–æ–∑–º–æ–∂–Ω–æ–π –†—É—Å—Å–∫–æ-—Ç—É—Ä–µ—Ü–∫–æ–π –≤–æ–π–Ω–µ (1849)"},
           {"event_id": "RU_FOREIGN_TURK_DIPLO", "event_name": "–î–∏–ø–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–∏—Å—Å–∏–∏ (–ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏–Ω–æ–ø–æ–ª—å)"} ]}
      ]
   },
  {"category_id": "OTHER", "category_name": "–î—Ä—É–≥–æ–µ / –ù–µ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ"}
]
"""
# –ó–∞–≥—Ä—É–∑–∫–∞ –ö–∞—Ä—Ç—ã –ó–Ω–∞–Ω–∏–π
try:
    KNOWLEDGE_MAP = json.loads(KNOWLEDGE_MAP_JSON)
    KM_CATEGORY_NAMES = {cat['category_id']: cat['category_name'] for cat in KNOWLEDGE_MAP}
    KM_SUBCATEGORY_NAMES = { subcat['subcategory_id']: subcat['subcategory_name'] for cat in KNOWLEDGE_MAP if 'subcategories' in cat for subcat in cat['subcategories'] }
    KM_EVENT_NAMES = { event['event_id']: event['event_name'] for cat in KNOWLEDGE_MAP if 'subcategories' in cat for subcat in cat['subcategories'] if 'events' in subcat for event in subcat['events'] }
    print("–ö–∞—Ä—Ç–∞ –ó–Ω–∞–Ω–∏–π —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞.")
except json.JSONDecodeError as e:
    st.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ö–∞—Ä—Ç—É –ó–Ω–∞–Ω–∏–π: {e}")
    KNOWLEDGE_MAP = []
    KM_CATEGORY_NAMES, KM_SUBCATEGORY_NAMES, KM_EVENT_NAMES = {}, {}, {}

# --- –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º ---
@st.cache_data
def load_data(pkl_path, csv_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ Pickle –∏–ª–∏ CSV."""
    df = None
    loaded_from = None
    if os.path.exists(pkl_path):
        try:
            df = pd.read_pickle(pkl_path)
            print(f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ pickle: '{pkl_path}'.")
            loaded_from = 'pickle'
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ pickle —Ñ–∞–π–ª–∞ '{pkl_path}': {e}. –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å CSV.")
            df = None
    else:
        st.warning(f"–§–∞–π–ª pickle '{pkl_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω.")

    if df is None:
        if os.path.exists(csv_path):
            st.warning(f"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å CSV: '{csv_path}'")
            try:
                # –£–∫–∞–∂–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–ª—è entry_id –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ, –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ
                dtype_spec = {'entry_id': 'Int64'} # –ò—Å–ø–æ–ª—å–∑—É–µ–º Int64 –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ NaN, –µ—Å–ª–∏ –≤–¥—Ä—É–≥ –±—É–¥—É—Ç
                df = pd.read_csv(csv_path, dtype=dtype_spec)
                print("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ CSV.")
                loaded_from = 'csv'
                try:
                    df.to_pickle(pkl_path)
                    print(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ pickle —Ñ–æ—Ä–º–∞—Ç–µ: '{pkl_path}'.")
                except Exception as pkl_save_error:
                    print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ pickle: {pkl_save_error}")
            except Exception as csv_load_error:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ CSV —Ñ–∞–π–ª–∞ '{csv_path}': {csv_load_error}")
                return pd.DataFrame()
        else:
             st.error(f"–†–µ–∑–µ—Ä–≤–Ω—ã–π CSV —Ñ–∞–π–ª '{csv_path}' —Ç–∞–∫–∂–µ –Ω–µ –Ω–∞–π–¥–µ–Ω.")
             return pd.DataFrame()

    if df is not None and not df.empty:
        try:
            # === –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ 'classification_id' –≤ 'event_id' ===
            if 'classification_id' in df.columns and 'event_id' not in df.columns:
                df.rename(columns={'classification_id': 'event_id'}, inplace=True)
                print("–ö–æ–ª–æ–Ω–∫–∞ 'classification_id' –ü–ï–†–ï–ò–ú–ï–ù–û–í–ê–ù–ê –≤ 'event_id'.")
            elif 'classification_id' in df.columns and 'event_id' in df.columns:
                 print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ü—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±–µ –∫–æ–ª–æ–Ω–∫–∏ 'classification_id' –∏ 'event_id'. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 'event_id'.")
                 # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: —É–¥–∞–ª–∏—Ç—å classification_id, –µ—Å–ª–∏ –æ–Ω–∞ —Ç–æ—á–Ω–æ –Ω–µ –Ω—É–∂–Ω–∞
                 # df.drop(columns=['classification_id'], inplace=True)
            elif 'event_id' not in df.columns:
                 print("–û—à–∏–±–∫–∞: –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'event_id' –∏–ª–∏ 'classification_id' –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏–∏.")
                 # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–æ–Ω–∫—É event_id –∏ –∑–∞–ø–æ–ª–Ω—è–µ–º N/A, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫ –Ω–∏–∂–µ
                 df['event_id'] = 'N/A'

            # 1. –î–∞—Ç—ã
            date_col_to_use = None
            if 'source_date' in df.columns: date_col_to_use = 'source_date'
            elif 'date' in df.columns: date_col_to_use = 'date'; print("Info: Using 'date' for 'source_date_dt'.")
            if date_col_to_use: df['source_date_dt'] = pd.to_datetime(df[date_col_to_use], errors='coerce')
            else: df['source_date_dt'] = pd.NaT; print("Error: No date column found.")

            # 2. –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            if 'keywords' in df.columns and loaded_from == 'csv':
                 print("–û–±—Ä–∞–±–æ—Ç–∫–∞ 'keywords' (–∏–∑ CSV)...")
                 def safe_literal_eval(val):
                    if isinstance(val, list): return val
                    try:
                        if pd.isna(val) or not isinstance(val, str): return []
                        if not val.strip() or val.strip() == '[]': return []
                        if val.startswith("'") and val.endswith("'"): val = val[1:-1]
                        if val.startswith('"') and val.endswith('"'): val = val[1:-1].replace('\\"', '"')
                        res = literal_eval(val)
                        return res if isinstance(res, list) else []
                    except: return []
                 df['keywords'] = df['keywords'].apply(safe_literal_eval)
            elif 'keywords' in df.columns:
                 def clean_keywords_list(lst):
                     if isinstance(lst, list): return [str(k).strip() for k in lst if k and isinstance(k, (str, int, float))]
                     return []
                 df['keywords'] = df['keywords'].apply(clean_keywords_list)

            # 3. Entry ID
            if 'entry_id' not in df.columns:
                print("Warning: Creating 'entry_id' from index.")
                df['entry_id'] = df.index
            df['entry_id'] = df['entry_id'].astype(int) # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —Ç–∏–ø int

            # 4. –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ NaN/–ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ –≤ event-–∫–æ–ª–æ–Ω–∫–∞—Ö –∑–Ω–∞—á–µ–Ω–∏–µ–º 'N/A'
            # –í–∫–ª—é—á–∞–µ–º 'event_id' –≤ —Å–ø–∏—Å–æ–∫, —á—Ç–æ–±—ã –æ–Ω —Ç–æ–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–ª—Å—è
            event_cols = ['event_id', 'event_name', 'description', 'date_in_text', 'location',
                          'historical_context', 'confidence', 'classification_confidence', 'text_fragment']
            for col in event_cols:
                if col in df.columns:
                    df[col] = df[col].fillna('N/A').replace([None, ''], 'N/A')
                else:
                    df[col] = 'N/A'
                    print(f"Warning: Added missing column '{col}' with 'N/A'.")

            # 5. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º event_id –≤ —Å—Ç—Ä–æ–∫—É –ü–û–°–õ–ï –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è NaN –∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è
            if 'event_id' in df.columns:
                df['event_id'] = df['event_id'].astype(str)
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å—Ç—Ä–æ–∫ 'nan', 'None', –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –ø–æ—è–≤–∏—Ç—å—Å—è –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
                df['event_id'] = df['event_id'].replace(['nan', 'None', 'null'], 'N/A', regex=False)

            print("–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
            return df
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return pd.DataFrame()
    else:
        return pd.DataFrame()


# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ö–∞—Ä—Ç—ã –ó–Ω–∞–Ω–∏–π (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ---
def get_categories():
    return [(cat['category_id'], cat['category_name']) for cat in KNOWLEDGE_MAP]

def get_subcategories(category_id):
    if not category_id: return []
    for cat in KNOWLEDGE_MAP:
        if cat['category_id'] == category_id and 'subcategories' in cat:
            return [(sub['subcategory_id'], sub['subcategory_name']) for sub in cat['subcategories']]
    return []

def get_events(subcategory_id):
    if not subcategory_id: return []
    for cat in KNOWLEDGE_MAP:
        if 'subcategories' in cat:
            for subcat in cat['subcategories']:
                if subcat['subcategory_id'] == subcategory_id and 'events' in subcat:
                    return [(ev['event_id'], ev['event_name']) for ev in subcat['events']]
    return []

def get_full_event_name(event_id):
    if not event_id or event_id == 'N/A' or event_id == 'OTHER' or pd.isna(event_id):
        return event_id
    event_id_str = str(event_id)
    if event_id_str in KM_EVENT_NAMES: return KM_EVENT_NAMES[event_id_str]
    if event_id_str in KM_SUBCATEGORY_NAMES: return KM_SUBCATEGORY_NAMES[event_id_str]
    if event_id_str in KM_CATEGORY_NAMES: return KM_CATEGORY_NAMES[event_id_str]
    return event_id_str

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---
df_merged = load_data(DATA_PATH_PKL, DATA_PATH_CSV)

# --- –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è ---
st.title("–ê–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π –≤ –¥–Ω–µ–≤–Ω–∏–∫–µ –≥–∏–º–Ω–∞–∑–∏—Å—Ç–∞ XIX –≤–µ–∫–∞ (v3.2)") # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Ä—Å–∏—é

if df_merged.empty:
    st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ.")
    st.stop()

# --- –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (–∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞) ---
# with st.expander("–û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"):
#     st.write("–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö:")
#     st.write(f"DataFrame –ø—É—Å—Ç–æ–π? {df_merged.empty}")
#     if not df_merged.empty:
#         st.write(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {len(df_merged)}")
#         st.write(f"–ö–æ–ª–æ–Ω–∫–∏: {df_merged.columns.tolist()}")
#         st.subheader("–ê–Ω–∞–ª–∏–∑ –∫–æ–ª–æ–Ω–∫–∏ event_id (–ø–æ—Å–ª–µ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è –∏ –æ—á–∏—Å—Ç–∫–∏)")
#         if 'event_id' in df_merged.columns:
#             unique_event_ids = df_merged['event_id'].unique()
#             st.write("–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ 'event_id':")
#             st.write(unique_event_ids)
#             st.write(f"–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –≤ 'event_id': {df_merged['event_id'].apply(type).value_counts()}")
#             st.write(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ 'N/A' –≤ 'event_id': {(df_merged['event_id'] == 'N/A').sum()}")
#             st.write(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ 'OTHER' –≤ 'event_id': {(df_merged['event_id'] == 'OTHER').sum()}")
#         else:
#             st.write("–ö–æ–ª–æ–Ω–∫–∞ 'event_id' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.")
#         st.write("–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫:")
#         st.dataframe(df_merged.head())

# --- –§–∏–ª—å—Ç—Ä—ã –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏ ---
st.sidebar.header("–§–∏–ª—å—Ç—Ä—ã –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è")

df_display = df_merged.copy()

# --- –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π —Ñ–∏–ª—å—Ç—Ä –ø–æ –ö–∞—Ä—Ç–µ –ó–Ω–∞–Ω–∏–π ---
st.sidebar.subheader("–§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Å–æ–±—ã—Ç–∏–π")
categories = get_categories()
category_options = [("ALL", "–í—Å–µ –ö–∞—Ç–µ–≥–æ—Ä–∏–∏")] + categories
selected_cat_tuple = st.sidebar.selectbox(
    "–ö–∞—Ç–µ–≥–æ—Ä–∏—è:", options=category_options, format_func=lambda x: x[1], key='cat_filter'
)
selected_cat_id = selected_cat_tuple[0] if selected_cat_tuple else None

subcat_disabled = not (selected_cat_id and selected_cat_id != "ALL")
subcategory_options = [("ALL", "–í—Å–µ –ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏")]
if not subcat_disabled: subcategory_options.extend(get_subcategories(selected_cat_id))

selected_subcat_tuple = st.sidebar.selectbox(
    "–ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è:", options=subcategory_options, format_func=lambda x: x[1], key='subcat_filter', disabled=subcat_disabled
)
selected_subcat_id = selected_subcat_tuple[0] if selected_subcat_tuple else None

event_disabled = not (selected_subcat_id and selected_subcat_id != "ALL")
event_options = [("ALL", "–í—Å–µ –°–æ–±—ã—Ç–∏—è")]
if not event_disabled: event_options.extend(get_events(selected_subcat_id))

selected_event_tuple = st.sidebar.selectbox(
    "–°–æ–±—ã—Ç–∏–µ:", options=event_options, format_func=lambda x: x[1], key='event_filter', disabled=event_disabled
)
selected_event_id = selected_event_tuple[0] if selected_event_tuple else None

# --- –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ —Ñ–∏–ª—å—Ç—Ä–∞ ---
# –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–æ —á—Ç–æ-—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ (–Ω–µ "ALL")
if selected_cat_id and selected_cat_id != "ALL":
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∞–º–æ–≥–æ –≥–ª—É–±–æ–∫–æ–≥–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è
    filter_id = selected_cat_id
    if selected_subcat_id and selected_subcat_id != "ALL":
        filter_id = selected_subcat_id
        if selected_event_id and selected_event_id != "ALL":
            filter_id = selected_event_id

    def check_hierarchical_id(event_id_val):
        if not isinstance(event_id_val, str) or event_id_val == 'N/A':
            return False
        # –î–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è (event_id) - —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if filter_id in KM_EVENT_NAMES:
            return event_id_val == filter_id
        # –î–ª—è –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ - —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–ª–∏ –Ω–∞—á–∞–ª–æ —Å—Ç—Ä–æ–∫–∏
        else:
            return event_id_val == filter_id or event_id_val.startswith(filter_id + '_')

    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä
    # –í–∞–∂–Ω–æ: —É–±–µ–¥–∏–º—Å—è, —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∞ event_id —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    if 'event_id' in df_display.columns:
        df_display = df_display[df_display['event_id'].apply(check_hierarchical_id)]
    else:
         st.sidebar.error("–û—à–∏–±–∫–∞: –ö–æ–ª–æ–Ω–∫–∞ 'event_id' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.")


# --- –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã ---
# (–ö–æ–¥ –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤ –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –æ–Ω –±—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω –∫ df_display)
st.sidebar.subheader("–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã")
# –î–∞—Ç–∞
date_col_present = 'source_date_dt' in df_display.columns and not df_display['source_date_dt'].isna().all()
if date_col_present and not df_display.empty:
    min_date_val = df_display['source_date_dt'].min()
    max_date_val = df_display['source_date_dt'].max()
    if pd.notna(min_date_val) and pd.notna(max_date_val):
        min_date, max_date = min_date_val.date(), max_date_val.date()
        if min_date <= max_date:
            try:
                date_range = st.sidebar.date_input( "–î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç –∑–∞–ø–∏—Å–µ–π:", value=(min_date, max_date), min_value=min_date, max_value=max_date, key='date_filter' )
                if len(date_range) == 2:
                    start_date, end_date = date_range
                    df_display = df_display[ df_display['source_date_dt'].notna() & (df_display['source_date_dt'].dt.date >= start_date) & (df_display['source_date_dt'].dt.date <= end_date) ]
            except: pass

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
keywords_col_present = 'keywords' in df_display.columns
if keywords_col_present and not df_display.empty:
    all_keywords_flat = [k for sublist in df_display['keywords'].dropna() if isinstance(sublist, list) for k in sublist if k and isinstance(k, str)]
    unique_keywords = sorted(list(set(all_keywords_flat)))
    if unique_keywords:
        selected_keywords = st.sidebar.multiselect("–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ò):", options=unique_keywords, key='keyword_filter')
        if selected_keywords:
            def check_keywords(kw_list): return isinstance(kw_list, list) and all(k in kw_list for k in selected_keywords)
            df_display = df_display[df_display['keywords'].apply(check_keywords)]

# –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è)
confidence_col_present = 'confidence' in df_display.columns
if confidence_col_present and not df_display.empty:
    confidence_levels = sorted([lvl for lvl in df_display['confidence'].dropna().unique() if lvl != 'N/A'])
    if confidence_levels:
        selected_confidence = st.sidebar.multiselect("–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (–ò–¥–µ–Ω—Ç.):", options=confidence_levels, key='confidence_filter')
        if selected_confidence: df_display = df_display[df_display['confidence'].isin(selected_confidence)]

# –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)
class_confidence_col_present = 'classification_confidence' in df_display.columns
if class_confidence_col_present and not df_display.empty:
    class_confidence_levels = sorted([lvl for lvl in df_display['classification_confidence'].dropna().unique() if lvl != 'N/A'])
    if class_confidence_levels:
        selected_class_confidence = st.sidebar.multiselect("–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (–ö–ª–∞—Å—Å.):", options=class_confidence_levels, key='class_confidence_filter')
        if selected_class_confidence: df_display = df_display[df_display['classification_confidence'].isin(selected_class_confidence)]


# --- –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ---
st.header("–û–±–∑–æ—Ä –∑–∞–ø–∏—Å–µ–π –∏ —Å–æ–±—ã—Ç–∏–π (–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ)")
display_columns = ['entry_id', 'date', 'event_id', 'event_name', 'description', 'location',
                   'confidence', 'classification_confidence', 'keywords']
actual_display_columns = [col for col in display_columns if col in df_display.columns]

if not df_display.empty:
    st.dataframe(df_display[actual_display_columns], height=300, use_container_width=True)
    st.write(f"–û—Ç–æ–±—Ä–∞–Ω–æ —Å—Ç—Ä–æ–∫: {len(df_display)}")
else:
    st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ñ–∏–ª—å—Ç—Ä–∞–º.")


# --- –î–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä –∑–∞–ø–∏—Å–∏ ---
st.header("–î–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä –∑–∞–ø–∏—Å–∏")

entry_id_present = 'entry_id' in df_display.columns
if not df_display.empty and entry_id_present:
    available_entry_ids = sorted(df_display['entry_id'].unique())
    if available_entry_ids:
        selected_entry_id = st.selectbox( "–í—ã–±–µ—Ä–∏—Ç–µ ID –∑–∞–ø–∏—Å–∏ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞:", options=available_entry_ids, key='entry_selector', index=None, placeholder="–í—ã–±–µ—Ä–∏—Ç–µ ID..." )
        if selected_entry_id is not None:
            entry_data_full = df_merged[df_merged['entry_id'] == selected_entry_id]
            if not entry_data_full.empty:
                diary_date = entry_data_full['date'].iloc[0] if 'date' in entry_data_full.columns else 'N/A'
                diary_text = entry_data_full['text'].iloc[0] if 'text' in entry_data_full.columns else '–¢–µ–∫—Å—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'

                st.subheader(f"–ó–∞–ø–∏—Å—å –æ—Ç {diary_date} (ID: {selected_entry_id})")
                st.markdown("#### –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∑–∞–ø–∏—Å–∏:")
                st.markdown(f"<div style='height: 300px; overflow-y: auto; border: 1px solid #ccc; padding: 10px; background-color: #f9f9f9; font-family: sans-serif; font-size: 14px;'>{diary_text}</div>", unsafe_allow_html=True)

                st.subheader("–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è:")
                events_in_entry = entry_data_full[ (entry_data_full['event_name'] != 'N/A') ].drop_duplicates(subset=['event_name', 'text_fragment'])

                if not events_in_entry.empty:
                    for i, (_, event_row) in enumerate(events_in_entry.iterrows()):
                        st.markdown("---")
                        event_name_display = event_row.get('event_name', 'N/A')
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º event_id, —Ç.–∫. –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–ª–∏ –∫–æ–ª–æ–Ω–∫—É
                        event_id_display = event_row.get('event_id', 'N/A')
                        full_hierarchical_name = get_full_event_name(event_id_display)

                        st.markdown(f"**–°–æ–±—ã—Ç–∏–µ {i+1}: {event_name_display}**")
                        with st.expander(f"–ü–æ–¥—Ä–æ–±–Ω–µ–µ: {event_name_display} (ID: {event_id_display})"):
                            if full_hierarchical_name != event_id_display and full_hierarchical_name not in ['N/A', 'OTHER']:
                                 st.markdown(f"**–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:** {full_hierarchical_name} (`{event_id_display}`)")
                            else:
                                 st.markdown(f"**–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è ID:** `{event_id_display}`")

                            st.markdown(f"**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (–ö–ª–∞—Å—Å.):** `{event_row.get('classification_confidence', 'N/A')}`")
                            st.markdown(f"**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (–ò–¥–µ–Ω—Ç.):** `{event_row.get('confidence', 'N/A')}`")
                            st.markdown(f"**–û–ø–∏—Å–∞–Ω–∏–µ:** {event_row.get('description', 'N/A')}")
                            st.markdown(f"**–î–∞—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–µ:** `{event_row.get('date_in_text', 'N/A')}`")
                            st.markdown(f"**–ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ:** `{event_row.get('location', 'N/A')}`")

                            keywords = event_row.get('keywords', [])
                            if isinstance(keywords, list) and keywords:
                                valid_keywords = [str(k).strip() for k in keywords if k and isinstance(k, (str, int, float)) and str(k).strip()]
                                if valid_keywords: st.markdown(f"**–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:** {' | '.join([f'`{k}`' for k in valid_keywords])}")
                            elif keywords != 'N/A': st.markdown(f"**–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:** `{keywords}`")

                            st.markdown(f"**–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç:**"); st.caption(f"{event_row.get('historical_context', 'N/A')}")
                            st.markdown(f"**–¶–∏—Ç–∏—Ä—É–µ–º—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞:**"); st.info(f"{event_row.get('text_fragment', 'N/A')}")
                else:
                    st.info("–î–ª—è —ç—Ç–æ–π –∑–∞–ø–∏—Å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π.")
            else:
                 st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–ø–∏—Å–∏ —Å ID {selected_entry_id}.")
    else:
        st.info("–ù–µ—Ç –∑–∞–ø–∏—Å–µ–π, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ–∫—É—â–∏–º —Ñ–∏–ª—å—Ç—Ä–∞–º, –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞.")
else:
    st.info("–ü—Ä–∏–º–µ–Ω–∏—Ç–µ —Ñ–∏–ª—å—Ç—Ä—ã –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –¥–µ—Ç–∞–ª–µ–π –∑–∞–ø–∏—Å–µ–π.")
    if not entry_id_present: st.warning("–ö–æ–ª–æ–Ω–∫–∞ 'entry_id' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö.")


# --- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: –í—Ä–µ–º–µ–Ω–Ω–∞—è —à–∫–∞–ª–∞ —Å–æ–±—ã—Ç–∏–π ---
st.header("–í—Ä–µ–º–µ–Ω–Ω–∞—è —à–∫–∞–ª–∞ —Å–æ–±—ã—Ç–∏–π")

timeline_possible = ('source_date_dt' in df_display.columns and
                     'event_name' in df_display.columns and # –ü—Ä–æ–≤–µ—Ä—è–µ–º event_name, —Ç.–∫. event_id –º–æ–∂–µ—Ç –±—ã—Ç—å N/A
                     not df_display.empty)

if timeline_possible:
    timeline_data = df_display[ (df_display['event_name'] != 'N/A') & df_display['source_date_dt'].notna() ].copy()
    if not timeline_data.empty:
        group_by_category = st.checkbox("–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∫–∞–ª–µ", key="group_timeline")
        timeline_data.set_index('source_date_dt', inplace=True)
        aggregation_period = st.selectbox( "–ü–µ—Ä–∏–æ–¥ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏:", options=['D', 'W', 'M', 'Q', 'Y'], index=2, format_func=lambda x: {'D':'–î–µ–Ω—å', 'W':'–ù–µ–¥–µ–ª—è', 'M':'–ú–µ—Å—è—Ü', 'Q':'–ö–≤–∞—Ä—Ç–∞–ª', 'Y':'–ì–æ–¥'}.get(x, x), key='timeline_agg' )
        try:
            if group_by_category and 'event_id' in timeline_data.columns:
                 # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–Ω—É—é –∫–æ–ª–æ–Ω–∫—É event_id
                 timeline_data['category_id'] = timeline_data['event_id'].apply( lambda x: str(x).split('_')[0] if isinstance(x, str) and '_' in str(x) else ('OTHER' if str(x) == 'OTHER' else 'UNKNOWN') )
                 timeline_data['category_id'] = timeline_data['category_id'].replace('UNKNOWN', 'OTHER')
                 timeline_counts = timeline_data.groupby([ pd.Grouper(freq=aggregation_period), 'category_id' ]).size().unstack(fill_value=0)
                 category_names_map = KM_CATEGORY_NAMES.copy()
                 if 'OTHER' not in category_names_map: category_names_map['OTHER'] = '–î—Ä—É–≥–æ–µ / –ù–µ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ'
                 timeline_counts.rename(columns=category_names_map, inplace=True)
            else:
                 timeline_counts = timeline_data.resample(aggregation_period).size()
                 timeline_counts = timeline_counts.rename("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π")
            if not timeline_counts.empty:
                st.line_chart(timeline_counts)
                grouping_text = " —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º" if group_by_category and 'category_id' in timeline_data.columns else ""
                st.caption(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º '{aggregation_period}'{grouping_text} (–Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞—Ç—ã –∑–∞–ø–∏—Å–∏).")
            else: st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∫–∞–ª—ã –ø–æ—Å–ª–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏.")
        except Exception as e: st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∫–∞–ª—ã: {e}"); st.exception(e)
    else: st.info("–ù–µ—Ç —Å–æ–±—ã—Ç–∏–π —Å –≤–∞–ª–∏–¥–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏ –≤ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∫–∞–ª—ã.")
else: st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∫–∞–ª—ã.")

# --- –ö–æ–Ω–µ—Ü –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è ---